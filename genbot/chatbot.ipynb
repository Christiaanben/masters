{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62dd7d76-0360-4104-99a3-537eecf367d0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3100bc-e8fc-4a85-9a84-e352b121df6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d2dcd6de70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split # scikit-learn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Constants\n",
    "RANDOM_SEED = 69\n",
    "DATASET_FILENAME = '../data/clean/customer_support_twitter_full.json'\n",
    "CLASSIFIER_MODEL_NAME = 'distilbert-base-uncased'\n",
    "GENERATOR_MODEL_NAME = 'microsoft/DialoGPT-small'\n",
    "SEQUENCE_LENGTH = 2\n",
    "\n",
    "# Setup\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1fc09f-7a6d-4458-ae5f-6c8f0878f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'classifier': {'input_ids': tensor([  101,  1030, 18108,  6279,  6442, 24471,  2140,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor([0., 0., 0., 0., 0., 0., 0., 0., 1.])}, 'predictor': (tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0.]]), tensor([1., 0., 1., 0., 1., 1., 0., 0., 0.])), 'generator': {'input_ids': tensor([   31, 16108, 15514,   220, 10289, 50256, 29904, 20608,   775,   821,\n",
      "          994,   329,   345,    13,  9022,  2196,   286,   262,  8969,   389,\n",
      "          345,  2491,    30,  6822,   422, 16163,  1875,  3611,  1875,  7994,\n",
      "           13, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0]), 'labels': tensor([ -100,  -100,  -100,  -100,  -100,  -100, 29904, 20608,   775,   821,\n",
      "          994,   329,   345,    13,  9022,  2196,   286,   262,  8969,   389,\n",
      "          345,  2491,    30,  6822,   422, 16163,  1875,  3611,  1875,  7994,\n",
      "           13, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
      "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257])}}]\n",
      "Conversation Count: 1001\n",
      "Label Counts: ['Acknowledgement', 'Call Center Inquiry', 'Check Version/Details', 'Direct to DM', 'Provide Information', 'Question', 'Report Problem', 'Troubleshooting', 'URL Share']\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, filename: str, batch_size=4):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.batch_size = batch_size\n",
    "        self.conversations = self._load_conversations(filename)\n",
    "        self.labels = self._determine_labels(self.conversations)\n",
    "        self.classifier_tokenizer = DistilBertTokenizerFast.from_pretrained(CLASSIFIER_MODEL_NAME)\n",
    "        self.generator_tokenizer = self._init_generator_tokenizer()\n",
    "\n",
    "        data = []\n",
    "        for conversation in self.conversations[:3]:\n",
    "            for j, (input_message, target_message) in enumerate(zip(conversation, conversation[1:])):\n",
    "                if target_message.get('authored'):\n",
    "                    data.append({\n",
    "                        'classifier': self._get_classifier_data(input_message),\n",
    "                        'predictor': self._get_predictor_data(conversation[:j], target_message),\n",
    "                        'generator': self._get_generator_data(input_message, target_message),\n",
    "                    })\n",
    "        print(data[:1])\n",
    "\n",
    "        # Split the data into 80% train, 10% validation, and 10% test\n",
    "        self.train_data, temp_data = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "        self.val_data, self.test_data = train_test_split(temp_data, test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "    @staticmethod\n",
    "    def _load_conversations(filename):\n",
    "        with open(filename) as file:\n",
    "            conversations = json.load(file)\n",
    "        return conversations\n",
    "\n",
    "    @staticmethod\n",
    "    def _determine_labels(conversations):\n",
    "        labels = set()\n",
    "        for conversation in conversations:\n",
    "            for message in conversation:\n",
    "                for intent in message.get('intents'):\n",
    "                    labels.add(intent)\n",
    "        return sorted(list(labels))\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_generator_tokenizer():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(GENERATOR_MODEL_NAME)\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        return tokenizer\n",
    "\n",
    "    def _get_label(self, intents: list[str]):\n",
    "        label = torch.zeros(len(self.labels))\n",
    "        for intent in intents:\n",
    "            label[self.labels.index(intent)] = 1\n",
    "        return label\n",
    "\n",
    "    def _get_classifier_data(self, message):\n",
    "        inputs = self.classifier_tokenizer(\n",
    "            message.get('text'), \n",
    "            padding='max_length',\n",
    "            max_length=50,\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': self._get_label(message.get('intents')),\n",
    "        }\n",
    "\n",
    "    def _get_predictor_data(self, sub_conversation, target_message):\n",
    "        labels = torch.stack(([self._get_label([])]*SEQUENCE_LENGTH)+[self._get_label(message.get('intents')) for message in sub_conversation])\n",
    "        latest_labels = labels[-SEQUENCE_LENGTH:]\n",
    "        return latest_labels, self._get_label(target_message.get('intents'))\n",
    "\n",
    "    def _get_generator_data(self, input_message, target_message):\n",
    "        text = f\"{input_message.get('text')}{self.generator_tokenizer.eos_token}{target_message.get('text')}\"\n",
    "        tokens = self.generator_tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=50,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids = tokens['input_ids'].squeeze()\n",
    "        start_idx = (input_ids == self.generator_tokenizer.eos_token_id).nonzero(as_tuple=True)[0][0]\n",
    "        labels = input_ids.clone()\n",
    "        labels[:start_idx+1] = -100\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': tokens['attention_mask'].squeeze(),\n",
    "            'labels': labels,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def stats(self):\n",
    "        return '\\n'.join([\n",
    "            f'Conversation Count: {len(self.conversations)}',\n",
    "            f'Label Counts: {self.labels}',\n",
    "        ])\n",
    "\n",
    "    @property\n",
    "    def n_labels(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "dm = MultiTaskDataModule(DATASET_FILENAME)\n",
    "print(dm.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c59aeb9-726b-4f62-b5b9-cee6c5eb7798",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class MultiTaskModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, n_labels):\n",
    "        super().__init__()\n",
    "        self.n_labels = n_labels\n",
    "\n",
    "        # DistilBERT Classifier\n",
    "        self.classifier = DistilBertForSequenceClassification.from_pretrained(\n",
    "            CLASSIFIER_MODEL_NAME,\n",
    "            num_labels=n_labels,\n",
    "            problem_type='multi_label_classification',\n",
    "            return_dict=True,\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "\n",
    "        # Next Intent Predictor\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size=n_labels, \n",
    "            hidden_size=self.classifier.config.dim,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.predictor_fc = torch.nn.Linear(self.classifier.config.dim, n_labels)\n",
    "\n",
    "        # Text Generator\n",
    "        self.generator = AutoModelForCausalLM.from_pretrained(GENERATOR_MODEL_NAME)\n",
    "        self.generator.resize_token_embeddings(len(dm.generator_tokenizer))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Classification\n",
    "        classifier_output = self.classifier(**inputs['classifier'])\n",
    "\n",
    "        # Predictor\n",
    "        classifier_hidden_state = classifier_output.hidden_states[-1][:,0,:]\n",
    "        h_0 = classifier_hidden_state.unsqueeze(dim=0).contiguous()\n",
    "        gru_output, _ = self.gru(inputs['predictor'][0], h_0)\n",
    "        # We're interested in the last output for prediction,\n",
    "        # which is the contextually richest. If x has shape (batch_size, seq_len, input_dim),\n",
    "        # out will have shape (batch_size, seq_len, hidden_dim).\n",
    "        # Thus, we select out[:,-1,:] to get a shape of (batch_size, hidden_dim)\n",
    "        gru_output = gru_output[:, -1, :]\n",
    "        fc_output = self.predictor_fc(gru_output)\n",
    "\n",
    "        # Generator\n",
    "        generator_output = self.generator(**inputs['generator'])\n",
    "        \n",
    "        return classifier_output, fc_output, generator_output\n",
    "\n",
    "    # def _common_step(self, batch, batch_idx):\n",
    "    #     classifier_output, predictor_output = self(batch)\n",
    "    #     return classifier_output\n",
    "\n",
    "    def _common_log(self, classifier_output, predictor_loss, generator_output):\n",
    "        self.log_dict({\n",
    "            'train_class_loss': classifier_output.loss,\n",
    "            'train_pred_loss': predictor_loss,\n",
    "            'train_gen_loss': generator_output.loss,\n",
    "        }, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        classifier_output, predictor_output, generator_output = self(batch)\n",
    "        pred_loss = F.binary_cross_entropy_with_logits(predictor_output, batch['predictor'][1])\n",
    "        self._common_log(classifier_output, pred_loss, generator_output)\n",
    "\n",
    "        total_loss = classifier_output.loss + pred_loss + generator_output.loss\n",
    "        return total_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=1e-5)\n",
    "        \n",
    "\n",
    "model = MultiTaskModel(dm.n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc16d21-f11c-4f20-b996-637d6cb77e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name         </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                                </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ classifier   │ DistilBertForSequenceClassification │ 67.0 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ gru          │ GRU                                 │  1.8 M │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2 </span>│ predictor_fc │ Linear                              │  6.9 K │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>│ generator    │ GPT2LMHeadModel                     │  124 M │\n",
       "└───┴──────────────┴─────────────────────────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                               \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ classifier   │ DistilBertForSequenceClassification │ 67.0 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ gru          │ GRU                                 │  1.8 M │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2\u001b[0m\u001b[2m \u001b[0m│ predictor_fc │ Linear                              │  6.9 K │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3\u001b[0m\u001b[2m \u001b[0m│ generator    │ GPT2LMHeadModel                     │  124 M │\n",
       "└───┴──────────────┴─────────────────────────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 193 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 193 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 772                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 193 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 193 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 772                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2398849c90fc4aa780c46606a0065683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\masters\\src\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "D:\\Documents\\masters\\src\\venv\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12d8fbc5d24492ea4046b11450087c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    callbacks=[pl.callbacks.RichProgressBar(leave=True)],\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e87a7f0a-9706-467a-9aae-8447981c5807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classifier': {'input_ids': tensor([[  101,  1030, 18108,  6279,  6442,  2023,  2003,  2054,  2009,  3504,\n",
      "          2066, 24471,  2140,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030, 18108,  6279,  6442,  2023,  2003,  2054,  2003,  6230,\n",
      "          1012,  1012,  1012, 24471,  2140,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'labels': tensor([[0., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 1.]])}, 'predictor': [tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0., 0.]]]), tensor([[0., 0., 0., 0., 0., 1., 0., 1., 0.],\n",
      "        [1., 0., 0., 1., 0., 0., 0., 1., 1.]])], 'generator': {}}\n",
      "{'classifier': {'input_ids': tensor([[  101,  1030, 18108,  6279,  6442,  1996, 14751, 10651,  1012,  1045,\n",
      "          2081,  2469,  2000,  8816,  2009,  7483,  1012,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1030, 18108,  6279,  6442,  2699, 25141,  3436,  2026, 10906,\n",
      "          1012,  1012, 23818,  2075,  2026,  3042,  1012,  1012,  2035,  2008,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]]), 'labels': tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0.]])}, 'predictor': [tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "         [1., 0., 1., 0., 1., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 1., 0.]]]), tensor([[0., 0., 0., 1., 0., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 0., 0., 1., 1.]])], 'generator': {}}\n"
     ]
    }
   ],
   "source": [
    "# print(model.classifier_tokenizer(d['input_text']))\n",
    "# X = [{'input_ids': model.classifier_tokenizer(d['input_text'])} for d in data]\n",
    "# data = [{'input_ids': [1,2,3]}, {'input_ids': [2,3,4]}]\n",
    "# data = [{'class': {'input_ids': [1,2,3]}}, {'class': {'input_ids': [2,3,4]}}]\n",
    "data = dm.train_data.copy()\n",
    "loader = DataLoader(data, batch_size=2)\n",
    "for batch in loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "913ee56e-30d7-4636-aa44-d0d3d888d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DistilBertForSequenceClassification.from_pretrained(CLASSIFIER_MODEL_NAME, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3e0e35d7-4f37-4045-a477-a5b89ad14456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier.config.dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128938b-dc46-426d-a439-faf3565ac054",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
