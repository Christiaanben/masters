{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bcf8e24-a8ed-455a-9aba-0b1a7e3fbeae",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the age of automation and digital interaction, crafting human-like responses in real-time is a challenge that has garnered immense attention. While there are numerous models built for understanding and generating human text, the real complexity arises when these models need to operate within a dynamic conversation flow, predicting user intents and crafting contextually relevant responses.\r\n",
    "\r\n",
    "This project is an exploration into the harmonious integration of two models. The first, our 'Predictor', is trained to anticipate the potential intent of a user message based on the preceding conversation. It doesn't just stop at understanding; it outputs specific labels that serve as guiding markers for the next step of our system.\r\n",
    "\r\n",
    "Enter the second part, DialoGPT. A variant of the powerful GPT (Generative Pre-trained Transformer) tailored for dialogues, DialoGPT has been making waves in the conversational AI community with its capabilities. However, in our use-case, it's not left to its own devices. Guided by the labels generated by our Predictor, DialoGPT's mission is to craft responses that aren't just coherent, but also contextually in sync with the predicted intent.\r\n",
    "\r\n",
    "By combining a targeted intent prediction mechanism with a state-of-the-art conversational model, we aim to bridge the gap between generic responses and those that resonate with the user's intent. This notebook chronicles our journey of integrating these models, fine-tuning DialoGPT on our dataset, and evaluating the outcomes of this symbiotic relatiolists.\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d32c9ca-2e6d-405b-bfca-13ae33cb1144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d5ae59eed0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "\n",
    "# Constants\n",
    "RANDOM_SEED = 69\n",
    "DATASET_FILENAME = '../data/clean/customer_support_twitter_full.json'\n",
    "MODEL_NAME = 'microsoft/DialoGPT-small'\n",
    "\n",
    "# Setup\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0097ae5-7001-4547-85dc-8dbda1beef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Count: 1001\n",
      "Sample Conversation:\n",
      "[\n",
      "  {\n",
      "    \"id\": 698,\n",
      "    \"text\": \"@AppleSupport  URL\",\n",
      "    \"authored\": false,\n",
      "    \"intents\": [\n",
      "      \"URL Share\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"id\": 696,\n",
      "    \"text\": \"USERNAME We're here for you. Which version of the iOS are you running? Check from Settings > General > About.\",\n",
      "    \"authored\": true,\n",
      "    \"intents\": [\n",
      "      \"Question\",\n",
      "      \"Provide Information\",\n",
      "      \"Check Version/Details\",\n",
      "      \"Acknowledgement\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Define PyTorch Dataset\n",
    "class GeneratorDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Define PyTorch Lightning DataModule\n",
    "class GeneratorDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, filename: str, batch_size=2):\n",
    "        super().__init__()\n",
    "        self.filename = filename\n",
    "        self.batch_size = batch_size\n",
    "        self.conversations = self._load_conversations(filename)\n",
    "        self.tokenizer = self._init_tokenizer()\n",
    "        data = []\n",
    "        for conv in self.conversations[:3]:\n",
    "            for input_message, target_message in zip(conv, conv[1:]):\n",
    "                if target_message.get('authored'):\n",
    "                    inputs = self.tokenizer(\n",
    "                        f\"{input_message.get('text')}\",\n",
    "                        padding='max_length',\n",
    "                        truncation=True,\n",
    "                        max_length=20,\n",
    "                        return_tensors='pt',\n",
    "                    )\n",
    "                    data.append({\n",
    "                        'input_ids': inputs['input_ids'].squeeze(),\n",
    "                        'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "                        'labels': inputs['input_ids'].squeeze(),\n",
    "                    })\n",
    "        \n",
    "        self.train_dataset = GeneratorDataset(data)\n",
    "        \n",
    "    @staticmethod\n",
    "    def _load_conversations(filename):\n",
    "        with open(filename) as file:\n",
    "            conversations = json.load(file)\n",
    "        return conversations\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_tokenizer():\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        return tokenizer\n",
    "\n",
    "    @property\n",
    "    def stats(self):\n",
    "        return '\\n'.join([\n",
    "            f'Conversation Count: {len(self.conversations)}',\n",
    "            # f'Label Counts: {self.labels}',\n",
    "            f'Sample Conversation:\\n{json.dumps(self.conversations[0][:2], indent=2)}',\n",
    "        ])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "\n",
    "dm = GeneratorDataModule(DATASET_FILENAME)\n",
    "print(dm.stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bc4080-3fb4-4e23-9f9d-7e4dbcf2de0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   31, 16108, 15514,   220, 10289, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([   31, 16108, 15514,   220, 10289, 50257, 50257, 50257, 50257, 50257,\n",
       "         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b497e802-cdae-42a2-890b-3060fdd44146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Loading & Configuration\n",
    "class GeneratorModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "        self.model.resize_token_embeddings(len(dm.tokenizer))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return self.model(**inputs)\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        outputs = self(batch)\n",
    "        return outputs.logits, batch['labels'], outputs.loss\n",
    "\n",
    "    def _common_log(self, outputs, labels, loss, stage: str):\n",
    "        self.log_dict({\n",
    "            f'{stage}_loss': loss,\n",
    "            f'{stage}_acc': self.accuracy(outputs, labels),\n",
    "            f'{stage}_f1': self.f1_score(outputs, labels),\n",
    "        }, prog_bar=True)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs, labels, loss = self._common_step(batch, batch_idx)\n",
    "        return loss\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return optim.AdamW(self.parameters(), lr=1e-5)\n",
    "\n",
    "model = GeneratorModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e038c7f-d15e-4186-a0b9-e6c0f296ecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type            </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ model │ GPT2LMHeadModel │  124 M │\n",
       "└───┴───────┴─────────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType           \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ model │ GPT2LMHeadModel │  124 M │\n",
       "└───┴───────┴─────────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 124 M                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 124 M                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 497                                                                        \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 124 M                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 124 M                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 497                                                                        \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155536a3449c4eb1879637d02de3f92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f8f3f5fbfc4e84b73cc509969e1b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training Loop\n",
    "logger = pl.loggers.TensorBoardLogger(\"runs\", name=\"generator\")\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2,\n",
    "    callbacks=[pl.callbacks.RichProgressBar(leave=True)],\n",
    "    logger=logger,\n",
    ")\n",
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63900e8a-5fbf-41cf-8179-785e3e31bbff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   39,  5037,   995, 50257, 50257, 50257, 50257, 50257, 50257, 50257]]), 'attention_mask': tensor([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.tokenizer('Hellow world', max_length=10, truncation=True, return_tensors='pt', padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c385c08e-00cb-48f9-b965-58d5aa3360e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
